# Quick Reference: Testing & Metrics

## üöÄ Get Started in 2 Minutes

```bash
cd /home/kartik/osel/monitor
source venv/bin/activate

# Option A: Quick smoke test (5‚Äì10 seconds)
python3 tools/collect_and_eval.py --duration 10 --outdir smoke_test

# Option B: Full benchmark with evaluation (60 seconds)
python3 tools/benchmark_eval.py --duration 60 --outdir benchmark_results

# View results
python3 tools/report_metrics.py benchmark_results
```

---

## üìä What You Get

### From `collect_and_eval.py`:
- ‚úÖ Metric samples (JSON)
- ‚úÖ EventBus statistics
- ‚úÖ Anomaly detections

### From `benchmark_eval.py`:
- ‚úÖ Metric samples + ground truth labels
- ‚úÖ Synthetic CPU spikes (via stress-ng)
- ‚úÖ **Precision, Recall, F1, ROC-AUC scores**
- ‚úÖ Detailed evaluation report

---

## üìà Expected Results (Benchmark)

| Metric | Value |
|--------|-------|
| **Events Published** | ~600 (60s √ó 10 events/sec) |
| **Dropped Events** | 0 |
| **Anomalies Detected** | 30‚Äì50 (depends on spike injection) |
| **Detection Coverage** | 70‚Äì80% (tunable) |
| **CPU Overhead** | <1% (monitoring process) |

---

## üéØ Evaluation Metrics Explained

### Precision
What fraction of *detected* anomalies were **true** spikes?
- **Formula**: TP / (TP + FP)
- **Target**: > 0.8 (few false alarms)

### Recall
What fraction of *true* spikes were **detected**?
- **Formula**: TP / (TP + FN)
- **Target**: > 0.7 (catch most spikes)

### F1-Score
Harmonic mean of precision & recall (0‚Äì1 scale).
- **Formula**: 2 √ó (Precision √ó Recall) / (Precision + Recall)
- **Target**: > 0.75 (good balance)

### ROC-AUC
Ranking quality of detection scores (0.5=random, 1.0=perfect).
- **Target**: > 0.80 (excellent discrimination)

---

## üîß Adjust Performance

### If Recall is Low (Missing Spikes)
```bash
# Lower detection threshold (less aggressive)
# ‚Üí Reduce window size
python3 tools/benchmark_eval.py --window 10 --spike-duration 10
```

### If Precision is Low (Too Many False Alarms)
```bash
# Raise detection threshold (more conservative)
# ‚Üí Increase window size
python3 tools/benchmark_eval.py --window 60 --spike-duration 15
```

### If Detection is Slow
```bash
# Faster detection loop (default 3s)
# ‚Üí Edit ml/anomaly_detection.py, adjust sleep(3.0)
```

---

## üìÅ Output Files Reference

| File | Contents | Example |
|------|----------|---------|
| `metrics.json` | Per-metric value arrays | `{"cpu.total": [10, 15, 20, ...]}` |
| `stats.json` | EventBus throughput | `{"total_events": 661, "dropped": 0}` |
| `detections.json` | Detected anomalies | `[{metric_key, confidence, method}, ...]` |
| `evaluation.json` | Precision/Recall/F1/AUC | `{precision: 0.8, recall: 0.75, ...}` |
| `ground_truth.json` | True spike labels | `{"cpu.total": [0,0,1,1,1,0,...]}` |

---

## üõ†Ô∏è Common Commands

```bash
# View metrics as table
cat benchmark_results/metrics.json | python3 -c \
  "import json, sys; m=json.load(sys.stdin); \
   print('\n'.join(f'{k}: {len(v)} samples' for k,v in m.items()))"

# Count anomalies
cat benchmark_results/detections.json | python3 -c \
  "import json, sys; d=json.load(sys.stdin); \
   print(f'Total detections: {len(d)}')"

# View evaluation metrics
python3 -m json.tool benchmark_results/evaluation.json | grep -E "Precision|Recall|F1|auc"

# Generate formatted report
python3 tools/report_metrics.py benchmark_results
```

---

## üö® Troubleshooting

| Problem | Solution |
|---------|----------|
| **"stress-ng not found"** | `sudo apt-get install stress-ng` |
| **"No module named 'ingestion'"** | `cd /home/kartik/osel/monitor` before running |
| **"Precision=0, Recall=0"** | Adjust `--window` size; try `--window 5` |
| **"No detections"** | Increase `--spike-duration` to 10‚Äì15s |
| **"Events=0"** | Collectors failed; check `psutil` is installed |

---

## üìã Test Matrix

Run multiple configurations to find optimal settings:

```bash
# Small spike, tight window
python3 tools/benchmark_eval.py --spike-duration 5 --window 5 --outdir test1

# Medium spike, medium window
python3 tools/benchmark_eval.py --spike-duration 10 --window 30 --outdir test2

# Large spike, loose window
python3 tools/benchmark_eval.py --spike-duration 15 --window 60 --outdir test3

# Compare results
for d in test*/; do echo "=== $d ==="; python3 tools/report_metrics.py "$d" | grep "cpu.total" -A10; done
```

---

## üìö Full Documentation

- **[TESTING_GUIDE.md](TESTING_GUIDE.md)** ‚Äì Detailed tool usage & workflows
- **[METRICS_AND_RESULTS.md](METRICS_AND_RESULTS.md)** ‚Äì Complete metrics report with comparisons
- **[README.md](docs/README.md)** ‚Äì Architecture & API overview
- **[ARCHITECTURE.md](docs/ARCHITECTURE.md)** ‚Äì Deep design patterns

---

## ‚úÖ Checklist: From Testing to Production

- [ ] Run smoke test: `python3 tools/collect_and_eval.py --duration 10`
- [ ] Run benchmark: `python3 tools/benchmark_eval.py --duration 60`
- [ ] Review metrics: `python3 tools/report_metrics.py benchmark_results`
- [ ] Tune thresholds if needed
- [ ] Test with real workload (not synthetic)
- [ ] Deploy: `sudo python3 main.py`
- [ ] Monitor API: `curl http://localhost:8000/stats`
- [ ] Setup alerting (if deploying long-term)

---

**Last Updated**: 2026-02-08  
**Version**: 1.0.0
